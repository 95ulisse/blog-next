export const meta = {
    title: "Let's talk about this async",
    date: "2022-05-08",
    tags: ["async", "rust"],
    desc: "This is a deep dive into the land of async rust. Get a drink, grab a snack, because this is going to be a long one",
}

This is a deep dive into the land of async rust. Get a drink, grab a snack, because this is going to be a long one.

## What the Async?

Async code exists to allow a programming language to offer more control in the way that code can run concurrently.
A famous example of an async programming language is JS, which can only execute code using 1 thread, but still needs
to support allowing multiple streams of work to happen at the same time.

For instance, say you want to make a network request, but while you're communicating with the external service, you want
to update the web page with a live updating progress bar.

TODO: insert joke about a progress bar while text shows stealing people's cookies, and mining crypto
<ProgressBar />

One way we can accomplish this in a single threaded setup is by allowing multiplexing.
That is, allowing one task to temporarily pause to allow another task to resume. Eventually, both tasks will be run to completion,
but neither blocks the other.

### I thought this blog post was about Rust...?

I've already hinted at another mechanism to allow concurrency, threads! Rust allows multithreading, so why do we need async code?
Well, there's many reasons. Maybe you're writing code on an embedded device that only supports a single threaded operations.
Maybe you're working in the Linux kernel itself and don't have access to just create threads as you please.
Maybe you just want more control about what tasks can run and in which order, and to not be at the mercy of your OS schedular.

Whatever your reasoning, async rust is general enough to support your needs

## Back to the Future

Before we dive to deep into async runtimes and how the OS handles IO etc, we need to talk about the future (trait).

Futures represent a contract that a value might not be ready immediately, but at a later point in time. Let's take a look at the definition

```rust
pub trait Future {
    type Output;
    fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output>;
}
```

There's a lot going on here... I don't want to talk about some of this yet, so for now, let's give it a bit of a hair cut...

```rust
pub trait Future2 /* eletric boogaloo */ {
    type Output;
    fn poll(&mut self) -> Poll<Self::Output>;
}
```

Ok, that looks a bit better, but what's this `Poll` thingy?

Well, it's ~~an Option~~ a representation that a future value is either ready, or not ready:

```rust
pub enum Poll<T> {
    Ready(T),
    Pending,
}
```

This is how our contract is upheld. If the future is ready to compute the value,
calling `poll` will return `Ready`, otherwise it won't, and instead returns `Pending`.

You'll also notice the poll function takes in a mut reference to itself, this means it can make progress on every individual poll function.
All this needs is someone to call that poll function for us.

## Baby's first Future

```rust
#[derive(Debug)]
enum MyFuture {
    Init,
    Step1(usize),
    Done,
}

impl Future2 for MyFuture {
    type Output = usize;
    fn poll(&mut self) -> Poll<Self::Output> {
        // take ownership of the current state, we pinky-promise to put it back
        let this = std::mem::replace(self, Self::Done);     // (4)

        let new = match this {
            Self::Init => Self::Step1(6),                   // (1)
            Self::Step1(n) => return Poll::Ready(7 * n),    // (3)
            Self::Done => panic!("please stop polling me"), // (5)
        };

        *self = new;                                        // (2)
        Poll::Pending
    }
}
```

This is an implementation of our simple Future trait over an enum state machine.
The state first computes the value of 6 (`1`), moving us into the `Step1` state (`2`).
On the next call, we'll compute the value of 7*6, where we return ready (`3`).
The state is set to `Done` by the first line (`4`).
Calling `poll` on a `Done` future should be a bug so we are free to panic (`5`).

Does it work? Let's try it!

```rust
fn main() {
    // initialise the future
    let mut fut = MyFuture::Init;
    let n = loop {
        // call poll
        match fut.poll() {
            // if pending, continue the loop
            Poll::Pending => {
                println!("value was not ready -> {fut:?}")
            }
            // if ready, break the poll loop with our value (no poll after ready, remember?!)
            Poll::Ready(n) => break n,
        }
    };
    // Done!
    println!("value is ready -> {n:?}");
}
```

```
   Compiling playground v0.0.1 (/playground)
    Finished dev [unoptimized + debuginfo] target(s) in 1.28s
     Running `target/debug/playground`

value was not ready -> Step1(6)
value is ready -> 42
```

Woo! We successfully made a future and polled it to completion

## Where's the async?

So, 2 problems. This doesn't do what we wanted.

First, where's the async stuff?! I thought this article was about async!

The second problem is that we've only got 1 future being polled.
The original point was to allow multiple tasks to be run concurrently.

I promise that we'll get back to the async later, but let's solve this second problem.
Scheduling multiple tasks together was the original point, so it'll be good if we can reason about that.

Solving this scheduling problem is easy actually, we just need more tasks.
Then, instead of calling `poll` on the same task over and over, we alternate.
The problem then becomes now how you interweve those tasks, but how you know
when best to poll a task.

For instance, let's say our future makes a HTTP request but the data is not ready yet.
There's no point constantly polling our future asking if it's ready, preferably it would tell us


### Time to add some more Context

Let's update our future trait again with one of the parts we removed

```rust
pub trait Future3 /* revenge of the context */ {
    type Output;
    fn poll(&mut self, cx: &mut Context<'_>) -> Poll<Self::Output>;
    //                 ^^^^^^^^^^^^^^^^^^^^  New!
}
```

This context is uhh, what is this? ~~Is this Go?~~

Ok, looking at the docs we see

```rust
/// The Context of an asynchronous task.
pub struct Context<'a> { /* private fields */ }

impl<'a> Context<'a> {
    /// Create a new Context from a &Waker.
    pub fn from_waker(waker: &'a Waker) -> Context<'a>;

    /// Returns a reference to the Waker for the current task.
    pub fn waker(&self) -> &'a Waker;
}
```

Ok, so `Context` clearly refers to our task, but it seems this `Waker` thing is more important.

```rust
/// A Waker is a handle for waking up a task by notifying its executor that it is ready to be run.
pub struct Waker { /* private fields */ }

impl Waker {
    /// Wake up the task associated with this Waker.
    pub fn wake(self);
}
```

Aha! So this seems to represent a task in an 'executor' (don't know what that is, whatever).
And it allows us to tell that task to wake up! This is what we want for our network interface.

We can create a future that talks to the OS for network access, and then we can get the OS to
call this `wake()` method when data is available to tell the task that it's ready to continue processing.


### Finally, some async

Ok ok, as promised, here is async code in it's full glory. Let's re-write our

```rust
async fn my_async() -> usize {
    let n = step1().await;
    7 * n
}

async fn step1() -> usize {
    6
}
```

This is the same as our `MyFuture` type from earlier,
but since it's written in a normal looking rust function, it can be easier to reason about.

We can see here that we await for the value of `step1` (6), and then we multiply it by 7 and return it.

Everywhere you see this `.await`, you can think of as a place our future state machine could return `Pending`.

Let's desugar a bit (you're already sweet enough)

Our async function is really a regular function that returns a future

```rust
fn my_async() -> impl Future<Output = usize> {
    async {
        let n = step1().await;
        7 * n
    }
}
```

And our async block will build the initial state of our future.

### Stick a Pin in it

Ok great! Our async/await can represent out Future types nicely. Maybe they can do more?

```rust
let x = "foo";
async {
    println!("{x}");
};
```

Ok awesome! Just like closures, async blocks can capture outside scope.
We should expect that this `Future` is bound by the lifetime of `x`.

This would be similar to the state machine

```rust
struct CaptureFuture<'x> {
    Init {
        x: &'x str
    }, // init -> done will print
    Done,
}
```

Ok, let's try something else. This time, maybe even a bit more realistic!
Let's say we have a `Vec<u8>` and we use an `AsyncRead` (works the same as `Read`, but async!).

```rust
async {
    let mut buf = Vec::new();

    let mut reader = TcpStream::new().await; // use your imagination

    let n = reader.read(&mut buf).await.unwrap();
    //             ^^^^           ^^^^^
    //               like read      but async :)

    println!("Read {:?}", buf[..n]);
}
```

This seems simple enough. Let's apply our magic and try and build this in our
state machine form.

First, let's disect this `.read()` future. It borrows our buffer mutably,
as well as the reader, so it must look something like

```rust
struct Read<'buf, 'read, R> {
    reader: &'read R
    buf: &'buf [u8]
    // more magic stuff here...
}
```

Ok, to start with,

```rust
enum TcpRead {
    // first, we create our buffer
    Init {
        buf: Vec<u8>
    },
    // then we create our reader to go along with it
    Reader {
        buf: Vec<u8>,
        reader: TcpStream,
    },
    // then we borrow buf and reader while we read...
    Reading {
        buf: Vec<u8>,
        reader: TcpStream,
        reading: Read<'buf, 'read, TcpStream>,
    },
    // then we print and finish
    Done,
}
```

Seems simple enough - wait a minute, what are the `'buf` and `'read` lifetimes here?!

This is a self referential enum! I'm sure this is a criminal offence in Rust, so what do we do here?
Maybe our async code is the problem? But we _really_ want this to work.

Ok, if we're commiting crimes, why don't we go all in?

So, self-referential data types are fine as long as you don't move them. The references will still point
to the same place in memory, which is still the data you expect.
However, if it moves, that's a problem since the pointers will be pointing at garbage data.

Right fine. So we can use unsafe to break our references a bit, make them static or something. We can then use

```rust
// Safety:
// Our caller pinky promises to not move our future (and therefore our buf)
let buf = unsafe {
    core::mem::transmute::<&mut [u8], &'static mut [u8]>(&mut buf);
};
```

That's great, but how do we tell our callers about this behaviour?
I guess we could make the `poll` method unsafe, but that kinda sucks if
there's lots of safe behaviours too.

Well, there is another way. What if we could make it unsafe to move the value _at all_?

That's what `Pin` is! `Pin` is a type that's unsafe to construct, unless our type implements
`Unpin` (an trait that guarantees it's not self-referential), or unless you use the `Box::pin` method to
pin it in a boxed allocation (since boxed data is guaranteed to not move).

This gives us our final `Future` trait:

```rust
pub trait Future {
    type Output;
    fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output>;
    //            ^^^ pinned! our callers have promised not to move us
}
```

### Executor Order 66

Ok, now that we have futures out of the way (sorry, that took a while) we can get to executors.

I showed off a simple executor earlier, let's update it to work with our new updated future trait:

```rust
fn main() {
    // We need a executor context now!
    // We'll see more on this later...
    let wake = MyExecutorWake::default();
    let waker = Waker::from(Arc::new(wake));
    let mut cx = Context::from_waker(&waker);

    // Initialise our future
    let mut fut = TcpRead::Init { buf: vec![] };

    // Pin the future
    // Safety: we'll be good and not move it until we stop polling it!
    let mut fut = unsafe { Pin::new_unchecked(&mut fut) };

    let n = loop {
        // call poll (as_mut does a reborrow for us so we can poll it again next time)
        match fut.as_mut().poll(&mut cx) {
            Poll::Ready(n) => break n,
            Poll::Pending => println!("value was not ready"),
        }
    };
    // Done!
    println!("value is ready -> {n:?}");
}
```

Ok, so what about spawning more tasks to run along side this one?

Well, I gave a solution earlier! We just keep a list of tasks and loop through them!

```rust
fn main() {
    /// snip: let cx = ...;

    // a ring buffer with efficient FIFO operations
    let mut tasks = VecDeque::new();

    // Pin the future (using box now because our tasks will be moving around :flushed:)
    let fut = Box::pin(/* fut */);

    // insert our task!
    tasks.push_back(fut);

    loop {
        let mut fut = tasks.pop_front().unwrap(); // take the first task
        match fut.as_mut().poll(&mut cx) {
            Poll::Ready(_) => break,
            Poll::Pending => {
                tasks.push_back(fut); // we need to re-queue our task now!
            },
        }
    };
}
```

Now, our `TcpRead` future just needs access to some `spawn()` function that pushes a second
task into our tasks queue.

Ok, let's extract it into a struct and make it a thread local instead,
that way we can easily call `spawn()` anywhere in our code stack!

```rust
// Task alias
type Task = Pin<Box<dyn Future<Output = ()>>>;

// our executor struct owns our task queue
#[derive(Default)]
pub struct Executor {
    // Using a RefCell because we'll need interior mutability :O
    queue: RefCell<VecDeque<Task>>
}

thread_local! {
    // Our thread local executor. Will be initialised later.
    // Uses an Rc because it will need multiple owners
    static EXECUTOR: OnceCell<Rc<Executor>> = OnceCell::new();
}

/// Spawns a future in our thread-local executor
pub fn spawn(&self, fut: impl Future<Output = ()>) {
    EXECUTOR.with(|e| e.get().unwrap().spawn(fut));
}

impl Executor {
    /// registers this executor onto the current thread
    fn register(self: &Rc<Self>) {
        EXECUTOR.with(|e| e.set(self.clone));
    }

    pub fn spawn(&self, fut: impl Future<Output = ()>) {
        self.tasks.borrow_mut().push_back(Box::pin(fut));
    }

    /// Waits for the future to complete
    pub fn block_on<F: Future>(&self, fut: F) -> F::Output {
        self.register(); // we're now in executor land

        // a way to store the output of the future, which will also signal
        // that we are done
        let output: Rc<RefCell<Option<F::Output>>> = Rc::new(RefCell::new(None));

        let output2 = Rc::clone(output);
        self.spawn(async move {
            let output = fut.await;
            *output2.borrow_mut() = Some(output); // set out output value
        });

        // I swear, I'll explain this junk in a little bit!
        let wake = MyExecutorWake::default();
        let waker = Waker::from(Arc::new(wake));
        let mut cx = Context::from_waker(&waker);

        loop {
            let mut fut = self.tasks.borrow_mut().pop_front().unwrap(); // take the first task
            if fut.as_mut().poll(&mut cx).is_pending() {
                self.tasks.borrow_mut().push_back(fut); // we need to re-queue our task now!
            }

            // exit our loop if we have our final value :)
            if let Some(output) = output.borrow_mut().take() {
                break output
            }
        }
    }
}
```

This is it! This is a functioning single threaded async executor. Let's run it

```rust
fn main() {
    let executor = Executor::default();
    executor.block_on(start());
}

async fn start() {
    // our magic business logic goes here
    for i in 0..10 {
        spawn(async move {
            println!("hello from task {i}");
        });
    }
}
```

### Iron Man want his `Arc<Reactor>` back
